# -*- coding: utf-8 -*-
"""ass2bcv_Q3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l80iCyj8xyuX0oTjnStqW8QJeT_RnfZ7
"""

import cv2
import numpy as np
from google.colab.patches import cv2_imshow
import imutils

# Read in the three images
img1 = cv2.imread('/content/1a.jpeg')
img2 = cv2.imread('/content/1b.jpeg')
img3 = cv2.imread('/content/1c.jpeg')
img3=imutils.rotate(img3, angle=4)
img2=cv2.resize(img2,(372,349))
img3=cv2.resize(img3,(372,349))
img1=cv2.resize(img1,(372,349))
img2=img2[0:372,110:349]
img3=img3[9:372,150:349]
# print(img1.shape)
print(img2.shape)
print(img3.shape)

# Convert the images to grayscale
gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
gray3 = cv2.cvtColor(img3, cv2.COLOR_BGR2GRAY)

# Create a SIFT object
sift = cv2.SIFT_create()

# Find the keypoints and descriptors for each image
kp1, des1 = sift.detectAndCompute(gray1, None)
kp2, des2 = sift.detectAndCompute(gray2, None)
kp3, des3 = sift.detectAndCompute(gray3, None)

# Create a matcher object
bf = cv2.BFMatcher()

# Match the descriptors of image1 and image2
matches1 = bf.knnMatch(des1, des2, k=2)

# Apply ratio test
good1 = []
for m, n in matches1:
    if m.distance < 0.7* n.distance:
        good1.append(m)

# Match the descriptors of image2 and image3
matches2 = bf.knnMatch(des2, des3, k=2)

# Apply ratio test
good2 = []
for m, n in matches2:
    if m.distance < 0.7 * n.distance:
        good2.append(m)

# Find the homography between image1 and image2
src_pts = np.float32([kp1[m.queryIdx].pt for m in good1]).reshape(-1, 1, 2)
dst_pts = np.float32([kp2[m.trainIdx].pt for m in good1]).reshape(-1, 1, 2)
M1, mask1 = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)

# Find the homography between image2 and image3
src_pts = np.float32([kp2[m.queryIdx].pt for m in good2]).reshape(-1, 1, 2)
dst_pts = np.float32([kp3[m.trainIdx].pt for m in good2]).reshape(-1, 1, 2)
M2, mask2 = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)

# matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)

# # Match descriptors.
# matches = matcher.match(des1, des2, None)  
# #Creates a list of all matches, just like keypoints
#   # Sort them in the order of their distance.
# matches = sorted(matches, key = lambda x:x.distance)



# result = cv2.drawMatches(img1,kp1, img2, kp2, matches[:10], None)
# Combine the homographies to get the overall homography
M = np.dot(M1,M2)

# Warp the images to create the panorama
pano_size = (img1.shape[1] + img2.shape[1] + img3.shape[1], img1.shape[0])
# w=img1.shape[1]+img2.shape[1]
# h=img1.shape[0]+img2.shape[0]
pano = cv2.warpPerspective(img1, M, pano_size)





# pano = cv2.warpPerspective(img1, M, pano_size)
pano[0:img1.shape[0], 0:img1.shape[1]] = img1

pano[0:img2.shape[0], img1.shape[1]:img1.shape[1] + img2.shape[1]] = img2
pano[0:img3.shape[0], img2.shape[1]+img1.shape[1]:img1.shape[1]+img2.shape[1] + img3.shape[1]] = img3
# # Save the panorama
# # result_image = cv2.warpPerspective(img1, M, (img1.shape[1] + img2.shape[1] + img3.shape[1], img1.shape[0]))

# # result_image[0:img2.shape[0], 0:img2.shape[1]] = img2

# result_image[:, img2.shape[1]:img2.shape[1] + img3.shape[1]] = cv2.warpPerspective(img3, M2, (img1.shape[1] + img2.shape[1] + img3.shape[1], img1.shape[0]))

cv2_imshow( pano)









