# -*- coding: utf-8 -*-
"""M22ma002_qu2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RX5joKpmLQkJm_EZZbCUsabWhRvlfqrb
"""

import os
# import wget
import zipfile
from pathlib import Path

dataset_folder = Path("Flickr_photo")
dataset_url = "https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip"
zip_file_path = dataset_folder / "Flickr8k_Dataset.zip"
extracted_folder_path = dataset_folder / "Flicker8k_Dataset"


print("Downloading Flickr dataset...")
!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip


# print("Extracting dataset...")
# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
#     zip_ref.extractall(dataset_folder)
!unzip -qq /content/Flickr8k_Dataset.zip

import requests
import zipfile
import io
from pathlib import Path

dataset_fold = Path("Flickr_caption")
data_link = "https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"


print("Downloading captions")
response = requests.get(data_link)
zip_file = zipfile.ZipFile(io.BytesIO(response.content))
zip_file.extractall(dataset_fold)
print("Done!")

import pandas as pd
from pathlib import Path
import cv2

data_path = Path("/content/Flicker8k_Dataset")
train_path = Path("/content/Flickr_caption/Flickr_8k.trainImages.txt")
capt_path = Path("/content/Flickr_caption/Flickr8k.token.txt")

output_csv_path = Path("/content/Flickr_caption/train_data.csv")

def create_traincsv(data_path, capt_path, output_csv_path):
    train_data_dict = {"image_path": [], "image_name": [], "caption": []}

    with open(capt_path) as captions_file:
        captions = captions_file.readlines()

    for line in captions:
        line = line.strip()
        img_name = line.split("\t")[0].split(".")[0]
        caption = line.split("\t")[1]
        img_path = data_path / f"{img_name}.jpg"

        if img_path.exists():
            train_data_dict["image_path"].append(str(img_path))
            train_data_dict["caption"].append(caption)
            train_data_dict["image_name"].append(img_name)

    train_data = pd.DataFrame(train_data_dict)
    train_data.to_csv(output_csv_path, index=False)
 


create_traincsv(data_path, capt_path, output_csv_path)

Train_csv=pd.read_csv("/content/Flickr_caption/train_data.csv")

Train_csv

class Vocabulary:
    def __init__(self):
        self.word2idx = {}
        self.idx2word = {}
        self.idx = 0

        # Add special tokens to the vocabulary
        self.add_word('<pad>')
        self.add_word('<start>')
        self.add_word('<end>')
        self.add_word('<unk>')

    def add_word(self, word):
        if not word in self.word2idx:
            self.word2idx[word] = self.idx
            self.idx2word[self.idx] = word
            self.idx += 1

    def __len__(self):
        return len(self.word2idx)


# Create a vocabulary object
vocab = Vocabulary()

# # Add words to the vocabulary
# caption = "a man is standing in front of a red car"
# tokens = caption.lower().split()
# for token in tokens:
#     vocab.add_word(token)

# # Print the size of the vocabulary
# print("Vocabulary size:", len(vocab))

import torchtext
from collections import Counter
import csv

# Define the tokenizer
tokenizer = torchtext.data.utils.get_tokenizer('spacy')

# Define the special tokens
special_tokens = ['<pad>', '<unk>', '<start>', '<end>']

# Load the captions from CSV file
captions = []
with open('/content/Flickr_caption/Flickr8k.token.txt', newline='') as csvfile:
    reader = csv.reader(csvfile)
    for row in reader:
        captions.append(row[0])

# Tokenize the captions and count the words
words = Counter()
for caption in captions:
    words.update(tokenizer(caption.lower()))

# Build the vocabulary
# vocab = torchtext.vocab.Vocab(words, specials=special_tokens, min_freq=5)

# Print the size of the vocabulary
print("Vocabulary size:",captions)

import os  
import pandas as pd  
import spacy  
import torch
from torch.nn.utils.rnn import pad_sequence  
from torch.utils.data import DataLoader, Dataset
import cv2
import torchvision.transforms as transforms

try:
    spacy_eng = spacy.blank("en")
except:
    os.system("python -m -q spacy download en")
finally:
    spacy_eng = spacy.blank("en")


class Vocabulary:
    def __init__(self, freq_threshold):
        self.word_to_index = {0: "<PAD>", 1: "<SOS>", 2: "<EOS>", 3: "<UNK>"}
        self.index_to_word = {"<PAD>": 0, "<SOS>": 1, "<EOS>": 2, "<UNK>": 3}
        self.freq_threshold = freq_threshold

    def __len__(self):
        return len(self.word_to_index)

    @staticmethod
    def tokenize_text(text):
        return text.lower().split()

    def build_vocabulary(self, text_list):
        word_frequencies = {}
        index = 4

        for text in text_list:
            for word in self.tokenize_text(text):
                if word not in word_frequencies:
                    word_frequencies[word] = 1
                else:
                    word_frequencies[word] += 1

                if word_frequencies[word] == self.freq_threshold:
                    self.index_to_word[index] = word
                    self.word_to_index[word] = index
                    index += 1

    def numericalize(self, text):
        tokenized_text = self.tokenize_text(text)
        return [self.word_to_index.get(token, self.word_to_index["<UNK>"]) for token in tokenized_text]



class FlickrDataset(Dataset):
    def __init__(self, root_folder, captions_file, transform=None):
        self.root_folder = root_folder
        self.df = pd.read_csv(captions_file)
        self.transform = transform

        self.images = self.df["image_name"]
        self.captions = self.df["caption"]

        self.vocab = Vocabulary(5)
        self.vocab.build_vocabulary(self.captions.tolist())

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        caption = self.captions[index]
        img_id = self.images[index] + ".jpg"
        img = cv2.imread(os.path.join(self.root_folder, img_id))
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        if self.transform is not None:
            img = self.transform(img)

        numericalized_caption = [self.vocab.index_to_word["<SOS>"]]
        numericalized_caption += self.vocab.numericalize(caption)
        numericalized_caption.append(self.vocab.index_to_word["<EOS>"])

        return img, torch.tensor(numericalized_caption)


class CustomCollate:
    def __init__(self, pad_index):
        self.pad_index = pad_index

    def __call__(self, batch):
        images = [item[0].unsqueeze(0) for item in batch]
        images = torch.cat(images, dim=0)
        targets = [item[1] for item in batch]
        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_index)

        return images, targets


def get_data_loader(
    root_folder,
    captions_file,
    transform,
    batch_size=64,
    num_workers=8,
    shuffle=True,
    pin_memory=True,
):
    dataset = FlickrDataset(root_folder, captions_file, transform=transform)
    pad_index = dataset.vocab.index_to_word["<PAD>"]
   
    loader = DataLoader(
        dataset=dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=shuffle,
        pin_memory=pin_memory,
        collate_fn=CustomCollate(pad_index=pad_index),
    )

    return loader, dataset

import os  
import pandas as pd  
import spacy  
import torch
from torch.nn.utils.rnn import pad_sequence  
from torch.utils.data import DataLoader, Dataset
from PIL import Image  
import torchvision.transforms as transforms

try:
    spacy_eng = spacy.blank("en")
except:
    os.system("python -m -q spacy download en")
finally:
    spacy_eng = spacy.blank("en")


class Vocabulary:
    def __init__(self, freq_threshold):
        self.word_to_index = {0: "<PAD>", 1: "<SOS>", 2: "<EOS>", 3: "<UNK>"}
        self.index_to_word = {"<PAD>": 0, "<SOS>": 1, "<EOS>": 2, "<UNK>": 3}
        self.freq_threshold = freq_threshold

    def __len__(self):
        return len(self.word_to_index)

    @staticmethod
    def tokenize_text(text):
        return text.lower().split()

    def build_vocabulary(self, text_list):
        frequencies = {}
        idx = 4

        for text in text_list:
            for word in self.tokenize_text(text):
                if word not in frequencies:
                    frequencies[word] = 1
                else:
                    frequencies[word] += 1

                if frequencies[word] == self.freq_threshold:
                    self.index_to_word[word] = idx
                    self.word_to_index[idx] = word
                    idx += 1

    def numericalize(self, text):
        tokenized_text = self.tokenize_text(text)
        return [self.index_to_word.get(token, self.index_to_word["<UNK>"]) for token in tokenized_text]

import os  
import pandas as pd  
import spacy  
import torch
import string
from torch.nn.utils.rnn import pad_sequence  
from torch.utils.data import DataLoader, Dataset
from PIL import Image  
import torchvision.transforms as transforms

try:
    spacy_eng = spacy.blank("en")
except:
    os.system("python -m -q spacy download en")
finally:
    spacy_eng = spacy.blank("en")

class Vocabu:
    

    @staticmethod
    def tokenize_text(text):
        # Use your preferred tokenizer implementation here
        # For example, let's split the text by whitespace and remove punctuation
        tokens = text.lower().split()
        tokens = [token.strip(string.punctuation) for token in tokens if token.strip(string.punctuation)]
        return tokens

class Voc:
    def __init__(self, freq_threshold):
        self.word_to_index = {0: "<PAD>", 1: "<SOS>", 2: "<EOS>", 3: "<UNK>"}
        self.index_to_word = {"<PAD>": 0, "<SOS>": 1, "<EOS>": 2, "<UNK>": 3}
        self.freq_threshold = freq_threshold

    def __len__(self):
        return len(self.word_to_index)
    @staticmethod
    def tokenize_text(text):
        return text.lower().split()

    def build_vocabulary(self, text_list):
        word_frequencies = {}
        index = 4

        for text in text_list:
            for word in self.tokenize_text(text):
                if word not in word_frequencies:
                    word_frequencies[word] = 1
                else:
                    word_frequencies[word] += 1

                if word_frequencies[word] == self.freq_threshold:
                    self.index_to_word[word] = word
                    self.word_to_index[idx] = index
                    index += 1

    def numericalize(self, text):
        tokenized_text = self.tokenize_text(text)
        return [self.word_to_index.get(token, self.word_to_index["<UNK>"]) for token in tokenized_text]



class FlickrDataset(Dataset):
    def __init__(self, root_folder, captions_file, transform=None):
        self.root_folder = root_folder
        self.df = pd.read_csv(captions_file)
        self.transform = transform

        self.images = self.df["image_name"]
        self.captions = self.df["caption"]

        self.vocab = Vocabulary(5)
        self.vocab.build_vocabulary(self.captions.tolist())

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        caption = self.captions[index]
        img_id = self.images[index] + ".jpg"
        img = Image.open(os.path.join(self.root_folder, img_id)).convert("RGB")

        if self.transform is not None:
            img = self.transform(img)

        numericalized_caption = [self.vocab.index_to_word["<SOS>"]]
        numericalized_caption += self.vocab.numericalize(caption)
        numericalized_caption.append(self.vocab.index_to_word["<EOS>"])

        return img, torch.tensor(numericalized_caption)


class CustomCollate:
    def __init__(self, pad_index):
        self.pad_index = pad_index
    # def __init__(self, pad_idx):
    #     self.pad_idx = pad_idx

    def __call__(self, batch):
        images = [item[0].unsqueeze(0) for item in batch]
        images = torch.cat(images, dim=0)
        targets = [item[1] for item in batch]
        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_index)

        return images, targets

class vocab2class:
    def __init__(self, pad_token_id):
        self.pad_token_id = pad_token_id

    def __call__(self, batch):
        images = [item[0].unsqueeze(0) for item in batch]
        images = torch.cat(images, dim=0)
        captions = [item[1] for item in batch]
        captions = pad_sequence(captions, batch_first=False, padding_value=self.pad_token_id)

        return images, captions



def getdata_loader(
    root_folder,
    captions_file,
    transform,
    batch_size=32,
    num_workers=8,
    shuffle=True,
    pin_memory=True,
):
    dataset = FlickrDataset(root_folder, captions_file, transform=transform)
    pad_index = dataset.vocab.index_to_word["<PAD>"]

    loader = DataLoader(
        dataset=dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=shuffle,
        pin_memory=pin_memory,
        collate_fn=CustomCollate(pad_index=pad_index),
    )

    return loader, dataset

import os  
import pandas as pd  
import spacy  
import torch
from torch.nn.utils.rnn import pad_sequence  
from torch.utils.data import DataLoader, Dataset
from PIL import Image  
import torchvision.transforms as transforms

try:
    spacy_eng = spacy.blank("en")
except:
    os.system("python -m -q spacy download en")
finally:
    spacy_eng = spacy.blank("en")


class Vocabulary:
    def __init__(self, freq_threshold):
        self.itos = {0: "<PAD>", 1: "<SOS>", 2: "<EOS>", 3: "<UNK>"}
        self.stoi = {"<PAD>": 0, "<SOS>": 1, "<EOS>": 2, "<UNK>": 3}
        self.freq_threshold = freq_threshold

    def __len__(self):
        return len(self.itos)

    @staticmethod
    def tokenize_text(text):
        return text.lower().split()

    def build_vocabulary(self, text_list):
        frequencies = {}
        idx = 4

        for text in text_list:
            for word in self.tokenize_text(text):
                if word not in frequencies:
                    frequencies[word] = 1
                else:
                    frequencies[word] += 1

                if frequencies[word] == self.freq_threshold:
                    self.stoi[word] = idx
                    self.itos[idx] = word
                    idx += 1

    def numericalize(self, text):
        tokenized_text = self.tokenize_text(text)
        return [self.stoi.get(token, self.stoi["<UNK>"]) for token in tokenized_text]



class FlickrDataset(Dataset):
    def __init__(self, root_folder, captions_file, transform=None):
        self.root_folder = root_folder
        self.df = pd.read_csv(captions_file)
        self.transform = transform

        self.images = self.df["image_name"]
        self.captions = self.df["caption"]

        self.vocab = Vocabulary(5)
        self.vocab.build_vocabulary(self.captions.tolist())

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        caption = self.captions[index]
        img_id = self.images[index] + ".jpg"
        img = Image.open(os.path.join(self.root_folder, img_id)).convert("RGB")

        if self.transform is not None:
            img = self.transform(img)

        numericalized_caption = [self.vocab.stoi["<SOS>"]]
        numericalized_caption += self.vocab.numericalize(caption)
        numericalized_caption.append(self.vocab.stoi["<EOS>"])

        return img, torch.tensor(numericalized_caption)


class CustomCollate:
    def __init__(self, pad_index):
        self.pad_index = pad_index
    # def __init__(self, pad_idx):
    #     self.pad_idx = pad_idx

    def __call__(self, batch):
        images = [item[0].unsqueeze(0) for item in batch]
        images = torch.cat(images, dim=0)
        targets = [item[1] for item in batch]
        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_index)

        return images, targets


def getdata_loader(
    root_folder,
    captions_file,
    transform,
    batch_size=64,
    num_workers=8,
    shuffle=True,
    pin_memory=True,
):
    dataset = FlickrDataset(root_folder, captions_file, transform=transform)
    pad_index = dataset.vocab.stoi["<PAD>"]

    loader = DataLoader(
        dataset=dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=shuffle,
        pin_memory=pin_memory,
        collate_fn=CustomCollate(pad_index=pad_index),
    )

    return loader, dataset

from torchvision import transforms
import torch
from torch.utils.data import DataLoader, random_split
import random

# Set random seeds for reproducibility
random.seed(42)
torch.manual_seed(42)

image_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

SPLIT_SIZE = 0.8
BATCH_SIZE = 64

loader, dataset = getdata_loader("/content/Flicker8k_Dataset/", "/content/Flickr_caption/train_data.csv", transform=image_transforms)

tr_data, test_data = random_split(dataset, [int(len(dataset) * SPLIT_SIZE), len(dataset) - int(len(dataset) * SPLIT_SIZE)])

train_loader = DataLoader(tr_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

print(f"Train data size: {len(tr_data)}")
print(f"Test data size: {len(test_data)}")
print(f"Train dataloader size: {len(train_loader)}")
print(f"Test dataloader size: {len(test_loader)}")

# visualizing the data
from matplotlib import pyplot as plt

plt.figure(figsize=(10,10))
for i in range(15,20):
    plt.subplot(2*2,5,i+1)
    plt.imshow(dataset[i][0].permute(1,2,0))
    plt.axis("off")
    
plt.show()

import torch
import torch.nn as nn
import torchvision.models as models
import cv2

device = "cuda" if torch.cuda.is_available() else "cpu"

class CNN_Encoder(nn.Module):
    def __init__(self, embed_size, train_CNN=False):
        super(CNN_Encoder, self).__init__()
        self.train_CNN = train_CNN
        self.resnet = models.resnet18(pretrained=True)
        
        for param in self.resnet.parameters():
            param.requires_grad = False
        
        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.52)

    def forward(self, images):
        features = self.resnet(images)
        return self.dropout(self.relu(features))


class RNN_Decoder(nn.Module):
    def __init__(self, embed_size, hidd_len, vocab_size, num_layers):
        super(RNN_Decoder, self).__init__()
        self.embed = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidd_len, num_layers)
        self.linear = nn.Linear(hidd_len, vocab_size)
        self.dropout = nn.Dropout(0.52)

    def forward(self, features, captions):
        embeddings = self.dropout(self.embed(captions))
        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)
        hiddens, _ = self.lstm(embeddings)
        outputs = self.linear(hiddens)
        return outputs


class ImageCaptModel(nn.Module):
    def __init__(self, embed_size, hidd_len, vocab_size, num_layers):
        super(ImageCaptModel, self).__init__()
        self.encoder = CNN_Encoder(embed_size)
        self.decoder = RNN_Decoder(embed_size, hidd_len, vocab_size, num_layers)

    def forward(self, images, captions):
        features = self.encoder(images)
        outputs = self.decoder(features, captions)
        return outputs

    def generate_caption(self, image, vocabulary, max_length=50):
        result_caption = []

        with torch.no_grad():
            features = self.encoder(image.unsqueeze(0).to(device))
            states = None

            for _ in range(max_length):
                hiddens, states = self.decoder.lstm(features, states)
                outputs = self.decoder.linear(hiddens.squeeze(0))
                predicted = outputs.argmax(-1)
                result_caption.append(predicted.item())
                features = self.decoder.embed(predicted).unsqueeze(0)

                if vocabulary.itos[predicted.item()] == "<EOS>":
                    break

        return [vocabulary.itos[idx] for idx in result_caption]

# Training the models
# num of epochs
num_epochs = 15

# model initialization
model = ImageCaptModel(embed_size=512, hidd_len=512, vocab_size=len(dataset.vocab), num_layers=1).to(device)


# loss function and optimizer
criterion  = torch.nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001,  weight_decay=0)
optimizer2=torch.optim.Adam(params=model.parameters(),lr=0.001)

from tqdm.auto import tqdm
device = "cuda" if torch.cuda.is_available() else "cpu"
device

import torch
import torch.nn as nn
import torchvision.models as models

# Loop over the dataset multiple times
loss_list = []
for epoch in tqdm(range(num_epochs)):
    epoch_loss = 0.0
    
    for idx, (images, captions) in tqdm(enumerate(loader), total=len(loader), leave=True):
        img = images.to(device)
        capt = captions.to(device)

        outputs = model(img, capt[:-1])
        losses = criterion(outputs.reshape(-1, outputs.shape[2]), capt.reshape(-1))
        epoch_loss += losses.item()
        
        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

    avg_epoch_loss = epoch_loss / len(loader)
    print(f"Epoch: {epoch}, Avg_Loss: {avg_epoch_loss:.4f}")
    loss_list.append(avg_epoch_loss)

# Save the model
torch.save(model.state_dict(), "image_capt_model.pth")

# loading the  model which was saved
model_captioning = ImageCaptModel(embed_size=512, hidd_len= 512, vocab_size=len(dataset.vocab), num_layers=1).to(device)
model_captioning.load_state_dict(torch.load("image_capt_model.pth"))
model_captioning.eval()

# getting the predictions
import matplotlib.pyplot as plt
plt.title("Training Loss Vs epochPlot")
plt.plot(loss_list)

plt.xlabel("Epochs-----X-axis")
plt.ylabel("Average Loss")
plt.show()

from tqdm.auto import tqdm
from nltk.translate.bleu_score import sentence_bleu

def evaluate(model, dataloader, vocabulary, device):
    total_loss, total_ppl = 0.0, 0.0
    total_bleu_score = 0.0
    total_sentences = 0

    for images, captions in tqdm(loader, total=len(loader), leave=True):
        images = images.to(device)
        captions = captions.to(device)

        outputs = model(images, captions[:-1])
        loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))

        total_loss += loss.item()
        total_ppl += torch.exp(loss).item()

        # Generate captions for evaluation
        # with torch.no_grad():
        #     generated_captions = model.generate_caption(images,dataset.vocab)

        # # Calculate BLEU score
        # for ref, gen in zip(captions.tolist(), generated_captions):
        #     reference = [vocabulary.itos[word] for word in ref if vocabulary.itos[word] not in ["<PAD>", "<EOS>"]]
        #     hypothesis = [vocabulary.itos[word] for word in gen if vocabulary.itos[word] not in ["<PAD>", "<EOS>"]]
        #     bleu_score = sentence_bleu([reference], hypothesis)
        #     total_bleu_score += bleu_score

        # total_sentences += len(captions)

    avg_loss = total_loss / len(dataloader)
    avg_pplty = total_ppl / len(dataloader)
    # avg_bleu_score = total_bleu_score / total_sentences

    return avg_loss, avg_pplty

avg_loss, perplext = evaluate(model_captioning, test_loader, dataset.vocab, device)
print(f"Loss: {avg_loss}, Perplexity: {perplext}")

def calculate_bleu_score(reference, candidate):
    reference = [reference]
    candidate = candidate
    return sentence_bleu(reference, candidate)

import matplotlib.pyplot as plt

for _ in range(7):
    reference_caption = []
    candidate_caption = []
    idx = torch.randint(0, len(test_data), ())
    img, caption = test_data[idx]
    img = img.to(device)
    
    plt.imshow(img.cpu().permute(1, 2, 0))
    plt.axis("off")
    plt.show()
    
    reference_caption = [dataset.vocab.itos[idx] for idx in caption.tolist()[1:-1]]
    candidate_caption = model_captioning.generate_caption(img, dataset.vocab)
    
    print("Actual Caption:")
    print(" ".join(reference_caption))
    
    print("Generated Caption:")
    print(" ".join(candidate_caption[1:len(candidate_caption)-1]))
    
    bleu_score = calculate_bleu_score(reference_caption, candidate_caption)
    print(f"BLEU Score: {bleu_score:.4f}\n")

